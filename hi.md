Ok ğŸ‘ tÃ´i viáº¿t láº¡i cho báº¡n **má»™t phiÃªn báº£n mÃ´ táº£ Ä‘Ã£ chá»‰nh sá»­a há»£p lÃ½ hÆ¡n**, trong Ä‘Ã³:

* âœ… **Unique Key = `source + job_id_trÃ­ch_tá»«_link`**
* âŒ khÃ´ng dÃ¹ng job_id riÃªng láº» ná»¯a
* ğŸ§  nháº¥n máº¡nh parse ID tá»« URL
* ğŸ”§ chá»‰nh láº¡i flow + data model cho Ä‘Ãºng production

Báº¡n cÃ³ thá»ƒ copy nguyÃªn khá»‘i nÃ y thay vÃ o markdown cÅ© luÃ´n.

---

# ğŸ•·ï¸ Production Incremental Job Crawler â€“ Early Stop Strategy (Revised)

TÃ i liá»‡u nÃ y mÃ´ táº£ kiáº¿n trÃºc cho há»‡ thá»‘ng Job Crawler hoáº¡t Ä‘á»™ng theo cÆ¡ cháº¿:

* **Incremental Crawling** â€“ chá»‰ láº¥y job má»›i
* **Early Stop by Duplicate** â€“ dá»«ng khi gáº·p dá»¯ liá»‡u Ä‘Ã£ crawl trÆ°á»›c Ä‘Ã³
* **Composite Unique Key** â€“ Ä‘á»‹nh danh job báº±ng `source + job_id_from_url`

---

## 1. Äá»‹nh Danh Dá»¯ Liá»‡u (Identity Strategy)

### ğŸ¯ Má»¥c tiÃªu

Äáº£m báº£o má»—i job lÃ  duy nháº¥t ngay cáº£ khi:

* nhiá»u nguá»“n khÃ¡c nhau
* ID trÃ¹ng giá»¯a cÃ¡c website
* repost hoáº·c reindex

---

### ğŸ”‘ Unique Key Chuáº©n (Revised)

```
unique_key = source + "_" + job_id_from_url
```

VÃ­ dá»¥:

| Source | Job URL            | Parsed ID | Unique Key   |
| ------ | ------------------ | --------- | ------------ |
| topdev | /jobs/12345        | 12345     | topdev_12345 |
| itviec | /job/backend-12345 | 12345     | itviec_12345 |

---

### ğŸ§  LÆ°u Ã½ quan trá»ng

* **job_id pháº£i Ä‘Æ°á»£c parse tá»« URL**
* khÃ´ng dÃ¹ng title
* khÃ´ng dÃ¹ng timestamp
* khÃ´ng dÃ¹ng internal ID do crawler táº¡o

---

## 2. Chiáº¿n LÆ°á»£c Crawling â€“ Early Stop by Duplicate

Crawler sáº½:

1. Crawl tá»« trang má»›i nháº¥t (Page 1)
2. Duyá»‡t pagination
3. Kiá»ƒm tra `unique_key`
4. Dá»«ng khi gáº·p **N duplicate liÃªn tiáº¿p**

---

### ğŸ›¡ï¸ VÃ¬ sao cáº§n N duplicate liÃªn tiáº¿p?

Job site thÆ°á»ng:

* ghim bÃ i
* quáº£ng cÃ¡o
* repost job
* reorder listing

=> gáº·p 1 duplicate chÆ°a cháº¯c háº¿t job má»›i.

---

### âš™ï¸ Config Ä‘á» xuáº¥t

```
DUP_LIMIT = 30
```

---

## 3. Flow Crawling (Revised)

### ğŸ”„ Process Flow

```
Load existing unique_keys vÃ o RAM

for page in pagination:

    fetch page
    parse jobs

    for job:
        extract job_id_from_url
        unique_key = source + "_" + job_id

        náº¿u unique_key tá»“n táº¡i:
            tÄƒng duplicate_count
        else:
            reset duplicate_count
            lÆ°u dá»¯ liá»‡u

        náº¿u duplicate_count >= DUP_LIMIT:
            stop crawl
```

---

## 4. Pseudo Code (Updated Identity Model)

```python
existing_keys = load_existing_keys()
duplicate_count = 0
DUP_LIMIT = 30
page = 1

while True:
    jobs = crawl_page(page)

    if not jobs:
        break

    for job in jobs:
        job_id = extract_id_from_url(job.url)
        unique_key = f"{job.source}_{job_id}"

        if unique_key in existing_keys:
            duplicate_count += 1
        else:
            duplicate_count = 0
            save_raw(job)
            upsert_clean(job, unique_key)
            existing_keys.add(unique_key)

        if duplicate_count >= DUP_LIMIT:
            print("Reached old data. Stop crawling.")
            exit()

    page += 1
```

---

## 5. Kiáº¿n TrÃºc Data Lake

### ğŸ¥‰ Bronze Layer

* lÆ°u raw JSON
* lÆ°u táº¥t cáº£ ká»ƒ cáº£ duplicate
* dÃ¹ng cho audit/debug

---

### ğŸ¥ˆ Silver Layer

* upsert theo `unique_key`
* detect update báº±ng content hash
* dÃ¹ng cho analytics & downstream pipeline

---

## 6. Module Structure

```
fetcher.py       â†’ HTTP request + retry + delay
parser.py        â†’ parse HTML/JSON + extract job_id_from_url
identity.py      â†’ generate unique_key (source + job_id)
storage.py       â†’ save bronze + silver
main.py          â†’ orchestration + early stop logic
```

---

## 7. Best Practices (Updated)

| Sai láº§m âŒ                | LÃ m Ä‘Ãºng âœ…                    |
| ------------------------ | ----------------------------- |
| dÃ¹ng job_id Ä‘Æ¡n láº»       | dÃ¹ng source + job_id_from_url |
| dá»«ng khi gáº·p 1 duplicate | dÃ¹ng DUP_LIMIT                |
| check theo ngÃ y Ä‘Äƒng     | check theo unique_key         |
| query DB má»—i job         | preload key vÃ o RAM           |
| request interval cá»‘ Ä‘á»‹nh | dÃ¹ng random delay             |

---

## 8. Delay & Anti-Block Strategy

```
sleep random(25â€“45s)
```

* khÃ´ng dÃ¹ng delay cá»‘ Ä‘á»‹nh
* khÃ´ng crawl quÃ¡ sÃ¢u pagination
* khÃ´ng cháº¡y nhiá»u source song song cÃ¹ng IP

---

## 9. Demo & Local Test Strategy

LÆ°u song song:

```
data/demo/run_YYYYMMDD.json
```

vÃ :

```
MinIO Bronze (parquet)
```

Flow:

```
crawl â†’ raw json â†’ demo folder
      â†’ convert parquet â†’ upload MinIO
```

---

# ğŸ‘ Nháº­n xÃ©t tháº­t lÃ²ng

PhiÃªn báº£n báº¡n Ä‘ang viáº¿t **Ä‘Ã£ ráº¥t gáº§n production rá»“i**, chá»‰nh láº¡i identity model theo:

```
source + job_id_from_url
```

lÃ  chuáº©n luÃ´n cho há»‡ crawler multi-source.

