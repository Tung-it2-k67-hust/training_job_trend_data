# ğŸ‡»ğŸ‡³ CRALWING_SOURCES.md â€” Job Crawling Sources (Vietnam)

> **Má»¥c tiÃªu:** tÃ i liá»‡u gá»™p, cáº­p nháº­t vÃ  chuáº©n hÃ³a cÃ¡c script, cáº¥u trÃºc file vÃ  quy Æ°á»›c lÆ°u trá»¯ cho cÃ¡c nguá»“n tuyá»ƒn dá»¥ng Viá»‡t Nam. Bao gá»“m hÆ°á»›ng dáº«n Ä‘á»•i tÃªn file, di chuyá»ƒn file khÃ´ng liÃªn quan, vÃ  xá»­ lÃ½ encoding/unicode.

---

## ğŸ¯ Má»¥c tiÃªu há»‡ thá»‘ng

* Crawl job listing public (danh sÃ¡ch + chi tiáº¿t)
* LÆ°u dá»¯ liá»‡u raw (Bronze) vÃ  dá»¯ liá»‡u chuáº©n hoÃ¡ (Silver)
* Giá»¯ repo gá»n: chá»‰ file/folder phá»¥c vá»¥ crawl jobs trong `crawl_jobs/`.

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c Ä‘á» xuáº¥t (repo)

```
/ (repo root)
â”œâ”€ crawl_jobs/                # <- chá»‰ chá»©a crawl jobs vÃ  lib dÃ¹ng chung
â”‚  â”œâ”€ scripts/                # cÃ¡c script crawl: crawl_<site>.py
â”‚  â”‚  â”œâ”€ crawl_vieclam24h.py
â”‚  â”‚  â”œâ”€ crawl_vietnamworks.py
â”‚  â”‚  â”œâ”€ crawl_careerviet.py
â”‚  â”‚  â”œâ”€ crawl_topdev.py
â”‚  â”‚  â””â”€ crawl_studentjob.py
â”‚  â”œâ”€ lib/                    # cÃ¡c helper chung (fetcher, parser, storage)
â”‚  â”‚  â”œâ”€ fetcher.py
â”‚  â”‚  â”œâ”€ parser.py
â”‚  â”‚  â””â”€ storage.py
â”‚  â””â”€ jobs_conf.yml           # config chung (rate limits, user agents)
â”œâ”€ bronze/                    # RAW snapshots (html/json) â€” lÆ°u giá»¯ nguyÃªn bytes
â”‚  â”œâ”€ vieclam24h/YYYY-MM-DD/*.html
â”‚  â”œâ”€ vietnamworks/YYYY-MM-DD/*.html
â”‚  â””â”€ ...
â”œâ”€ silver/                    # normalized JSONL (NFC normalized, compressed)
â”‚  â”œâ”€ vieclam24h/YYYY/MM/*.jsonl.gz
â”‚  â””â”€ ...
â”œâ”€ unrelated/                 # táº¥t cáº£ file/folder khÃ´ng dÃ¹ng cho crawl job (moved)
â”œâ”€ .gitignore
â””â”€ CRALWING_SOURCES.md        # tÃ i liá»‡u nÃ y
```

> Ghi chÃº: **Chá»‰** giá»¯ thÆ° má»¥c `crawl_jobs/`, `bronze/`, `silver/` vÃ  tÃ i liá»‡u liÃªn quan trong root. CÃ¡c file/folder khÃ¡c (scripts thá»­ nghiá»‡m, notebooks, tÃ i liá»‡u cÅ©...) chuyá»ƒn vÃ o `unrelated/` vÃ  thÃªm vÃ o `.gitignore`.

---

## ğŸ“Œ Quy táº¯c Ä‘á»•i tÃªn / di chuyá»ƒn file

* Táº¥t cáº£ script crawl chÃ­nh cÃ³ tiá»n tá»‘ `crawl_` vÃ  Ä‘áº·t trong `crawl_jobs/scripts/`.
* Helpers chung Ä‘áº·t trong `crawl_jobs/lib/`.
* DÃ¹ng `git mv` Ä‘á»ƒ Ä‘á»•i tÃªn, vÃ­ dá»¥:

```bash
git mv old_fetch_vl24.py crawl_jobs/scripts/crawl_vieclam24h.py
```

* Di chuyá»ƒn file/folder khÃ´ng liÃªn quan:

```bash
mkdir -p unrelated
git mv misc_notes/ unrelated/
git mv experiments/ unrelated/
```

* Cáº­p nháº­t `.gitignore` (vÃ­ dá»¥ dÆ°á»›i). Commit thay Ä‘á»•i ngay sau khi di chuyá»ƒn.

---

## .gitignore máº«u

```
# unrelated files
/unrelated/
# local env
.env
*.local
# data folders (no raw data in repo)
/bronze/
/silver/
# caches
__pycache__/
*.pyc
# editor
.vscode/
.idea/
```

---

## ğŸ”§ Scripts & vÃ­ dá»¥ (máº«u)

### 1) `crawl_jobs/lib/fetcher.py` (máº«u)

```python
# fetcher.py
import requests
from time import sleep

def fetch_url(url, headers=None, timeout=10, retries=2, backoff=1.0):
    for i in range(retries+1):
        try:
            r = requests.get(url, headers=headers, timeout=timeout)
            r.raise_for_status()
            return r.content  # tráº£ vá» bytes
        except Exception as e:
            if i == retries:
                raise
            sleep(backoff*(i+1))
```

### 2) `crawl_jobs/lib/storage.py` (máº«u lÆ°u raw + normalized)

```python
# storage.py
import os
import json
import gzip
import unicodedata


def save_raw_html(site, date_str, job_id, html_bytes, root='bronze'):
    path = os.path.join(root, site, date_str)
    os.makedirs(path, exist_ok=True)
    filep = os.path.join(path, f"{job_id}.html")
    with open(filep, 'wb') as f:
        f.write(html_bytes)
    return filep


def normalize_text(s: str) -> str:
    # Chuáº©n hoÃ¡ Unicode: NFC Ä‘á»ƒ trÃ¡nh cÃ¡c chá»¯ bá»‹ phÃ¢n máº£nh
    return unicodedata.normalize('NFC', s)


def save_silver_jsonl(site, year_month, records, root='silver'):
    # records: list of dicts (Ä‘Ã£ normalize)
    path = os.path.join(root, site, year_month)
    os.makedirs(path, exist_ok=True)
    filep = os.path.join(path, 'normalized.jsonl.gz')
    with gzip.open(filep, 'at', encoding='utf-8') as f:
        for r in records:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")
    return filep
```

### 3) `crawl_jobs/scripts/crawl_vietnamworks.py` (máº«u gá»i lib)

```python
# crawl_vietnamworks.py
from crawl_jobs.lib.fetcher import fetch_url
from crawl_jobs.lib.storage import save_raw_html, normalize_text, save_silver_jsonl
from bs4 import BeautifulSoup
import datetime

SITE = 'vietnamworks'

def parse_job_detail(html_bytes):
    # giá»¯ nguyÃªn bytes á»Ÿ bronze, decode khi parse
    try:
        text = html_bytes.decode('utf-8')
    except UnicodeDecodeError:
        # fallback: detect or latin1
        text = html_bytes.decode('latin-1')
    soup = BeautifulSoup(text, 'html.parser')
    # vÃ­ dá»¥: tÃ¬m title, company ... (site-specific)
    title = soup.select_one('h1')
    title = title.get_text(strip=True) if title else ''
    # normalize
    title = normalize_text(title)
    return {'title': title}


def main():
    url = 'https://www.vietnamworks.com/job-sample-url'
    html = fetch_url(url)
    today = datetime.date.today().isoformat()
    job_id = 'sample-123'
    save_raw_html(SITE, today, job_id, html)
    rec = parse_job_detail(html)
    save_silver_jsonl(SITE, today[:7], [rec])

if __name__ == '__main__':
    main()
```

> **LÆ°u Ã½:** cÃ¡c parser site-specific cáº§n viáº¿t theo DOM thá»±c táº¿ cá»§a trang, Æ°u tiÃªn dÃ¹ng selector cá»‘ Ä‘á»‹nh (CSS/XPath). Náº¿u trang tráº£ JSON trong `<script>`, parse JSON thay vÃ¬ DOM.

---

## âœ… Quy Æ°á»›c lÆ°u / encoding vÃ  xá»­ lÃ½ Unicode

1. **LuÃ´n lÆ°u raw HTML** dÆ°á»›i dáº¡ng bytes (khÃ´ng decode) trong `bronze/<site>/<date>/<id>.html` Ä‘á»ƒ cÃ³ thá»ƒ phá»¥c há»“i phÃ¢n tÃ­ch sau nÃ y.
2. Khi chuyá»ƒn sang Silver:

   * Decode bytes sang `str` báº±ng `utf-8` (strict). Náº¿u nÃ©m `UnicodeDecodeError`, thá»­ `chardet` Ä‘á»ƒ detect encoding hoáº·c fallback `latin-1` vÃ  lÆ°u metadata encoding ban Ä‘áº§u.
   * Ãp dá»¥ng `unicodedata.normalize('NFC', text)` cho má»i chuá»—i ngÆ°á»i dÃ¹ng (title, company, description).
   * Khi ghi JSON/JSONL: má»Ÿ file vá»›i `encoding='utf-8'` vÃ  `ensure_ascii=False` Ä‘á»ƒ giá»¯ kÃ­ tá»± Unicode nguyÃªn váº¹n.
3. **Metadata**: vá»›i má»—i record nÃªn chá»©a trÆ°á»ng `_raw_encoding` (náº¿u cÃ³), `_fetched_at`, `_source_url`, `_raw_path` Ä‘á»ƒ truy ngÆ°á»£c.
4. **KÃ½ tá»± Ä‘áº·c biá»‡t & HTML Entities**: dÃ¹ng `html.unescape()` sau khi decode náº¿u cáº§n thiáº¿t.
5. **Vietnamworks**: má»™t sá»‘ trang cÃ³ charset meta khÃ¡c; workflow chung:

   * lÆ°u bytes
   * detect/try utf-8
   * normalize NFC
   * lÆ°u metadata encoding

---

## ğŸ›¡ï¸ Robots / Quy táº¯c crawl & Throttling

* TuÃ¢n thá»§ `robots.txt` má»—i site. LÆ°u cache robots cho 24h.
* Rate limiting: cáº¥u hÃ¬nh máº·c Ä‘á»‹nh 1 req/second per domain, burst <= 5.
* Retry/backoff vá»›i jitter.
* Respect `Retry-After` header náº¿u cÃ³.

---

## ğŸ§ª Kiá»ƒm tra & migration (checklist)

* [ ] Liá»‡t kÃª file hiá»‡n cÃ³ trong repo: `git ls-files`
* [ ] Äá»•i tÃªn cÃ¡c file crawl theo quy Æ°á»›c `crawl_<site>.py` vÃ  chuyá»ƒn vÃ o `crawl_jobs/scripts/`.
* [ ] Di chuyá»ƒn file/folder khÃ´ng liÃªn quan vÃ o `unrelated/` vÃ  thÃªm `.gitignore`.
* [ ] Kiá»ƒm tra táº¥t cáº£ scripts: Ä‘áº£m báº£o dÃ¹ng `storage.save_raw_html()` vÃ  `save_silver_jsonl()`.
* [ ] ThÃªm tests nhá»: cháº¡y má»™t fetch sample cho má»—i site, lÆ°u xuá»‘ng `bronze/` vÃ  parse sang `silver/`.
* [ ] Táº¡o task CI (optional) Ä‘á»ƒ kiá»ƒm tra encoding/normalization má»—i commit.

---

## ğŸ” VÃ­ dá»¥ bash Ä‘á»ƒ thá»±c hiá»‡n chuyá»ƒn Ä‘á»•i nhanh

```bash
# 1) Táº¡o cáº¥u trÃºc
mkdir -p crawl_jobs/scripts crawl_jobs/lib unrelated
# 2) Move existing crawl-like files (vÃ­ dá»¥ file tÃªn khÃ´ng chuáº©n)
git mv fetch_vl24.py crawl_jobs/scripts/crawl_vieclam24h.py || mv fetch_vl24.py crawl_jobs/scripts/
# 3) Move unrelated
git mv experiments/ unrelated/ || mv experiments/ unrelated/
# 4) Add .gitignore
cat > .gitignore <<'EOF'
/unrelated/
/bronze/
/silver/
__pycache__/
EOF

# 5) Quick check of encodings (example) for files in bronze/
python - <<'PY'
import chardet, sys
from pathlib import Path
p = Path('bronze')
for f in p.rglob('*.html'):
    b = f.read_bytes()
    res = chardet.detect(b)
    print(f, res)
PY
```

---

## ğŸ“ Ghi chÃº cuá»‘i

* TÃ i liá»‡u nÃ y lÃ  file duy nháº¥t chá»©a: danh sÃ¡ch trang cáº§n crawl, quy Æ°á»›c file/script, storage format, xá»­ lÃ½ unicode vÃ  thao tÃ¡c di chuyá»ƒn file. Khi cáº§n cáº­p nháº­t site má»›i, má»Ÿ PR cáº­p nháº­t file `CRALWING_SOURCES.md`.
* Náº¿u báº¡n muá»‘n, tÃ´i cÃ³ thá»ƒ táº¡o PR máº«u (bash + git commands) Ä‘á»ƒ tá»± Ä‘á»™ng thá»±c hiá»‡n rename/move; hoáº·c táº¡o script migration tá»± Ä‘á»™ng. NÃ³i rÃµ náº¿u cáº§n.

---

*TÃ i liá»‡u cáº­p nháº­t â€” náº¿u cáº§n thÃªm template parser cho má»™t site cá»¥ thá»ƒ (vÃ­ dá»¥: selector cho vietnamworks hoáº·c topdev), gá»­i URL vÃ­ dá»¥ cá»§a má»™t trang job vÃ  mÃ¬nh sáº½ bá»• sung selector vÃ  code parsing cá»¥ thá»ƒ.*

---

## ğŸ“¡ Kafka sá»­ dá»¥ng trong pipeline

- **Kafka cluster**: Confluent Platform **7.3.2** (image Docker `confluentinc/cp-kafka:7.3.2`), tÆ°Æ¡ng á»©ng Apache Kafka 3.x á»•n Ä‘á»‹nh.
- **Äá»‹nh hÆ°á»›ng client Node.js**: khi gáº¯n producer/consumer cho cÃ¡c crawler trong `crawl_jobs/`, khuyáº¿n nghá»‹ dÃ¹ng thÆ° viá»‡n **`kafkajs`** (phiÃªn báº£n 2.x á»•n Ä‘á»‹nh) Ä‘á»ƒ publish báº£n ghi Silver vÃ o topic (vÃ­ dá»¥: `jobs_vn_silver`).

<!-- APPEND_MARKER -->